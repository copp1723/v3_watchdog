# Sandboxed Agent Execution System

This document provides an overview of the sandboxed agent execution system for Watchdog AI, which enables secure execution of LLM-generated code with strict schema validation.

## Overview

The sandboxed agent execution system allows executing Python code (typically generated by an LLM) in a secure, isolated environment with resource limits and restrictions to prevent malicious code execution. The system ensures the output conforms to a predefined JSON schema and provides robust error handling and retry mechanisms.

## Key Features

- **Secure Execution**: Runs code in a separate process with memory limits and timeout constraints
- **Resource Restrictions**: Limits available modules and prevents access to system resources
- **Schema Validation**: Enforces a strict JSON schema contract on the output
- **Automatic Retries**: Supports retrying with modified prompts when execution fails
- **Comprehensive Logging**: Logs all aspects of code execution for auditability
- **Error Handling**: Provides detailed error information for debugging

## Core Components

### 1. SandboxConfig

Configuration settings for the sandbox execution environment:

```python
from src.utils.agent_sandbox import SandboxConfig

config = SandboxConfig(
    memory_limit_mb=512,  # Memory limit in MB
    execution_timeout_seconds=10,  # Maximum execution time in seconds
    enable_network=False,  # Whether to allow network access
    allowed_modules=["pandas", "numpy", "math"]  # Explicitly allowed modules
)
```

### 2. code_execution_in_sandbox

The main function for executing code in the sandbox:

```python
from src.utils.agent_sandbox import code_execution_in_sandbox

result = code_execution_in_sandbox(
    code="# Python code to execute\navg = df['sales'].mean()\nresult = {'answer': f'Average: {avg}', 'data': {'avg': avg}, 'chart_type': 'none', 'confidence': 0.9}",
    df=dataframe,  # pandas DataFrame to analyze
    schema=output_schema,  # Optional JSON schema (uses default if not provided)
    llm_service_func=generate_code,  # Function to call LLM for retries
    original_prompt="What's the average sales?",  # Original prompt for retries
    enable_retry=True,  # Whether to retry on failure
    config=config  # Sandbox configuration
)
```

### 3. Output Schema

The system enforces a JSON schema on the output to ensure consistent structure:

```python
DEFAULT_OUTPUT_SCHEMA = {
    "type": "object",
    "required": ["answer", "data", "chart_type", "confidence"],
    "properties": {
        "answer": {
            "type": "string",
            "description": "Textual answer to the user's question"
        },
        "data": {
            "type": ["object", "array"],
            "description": "JSON-serializable data for visualization"
        },
        "chart_type": {
            "type": "string",
            "enum": ["table", "bar", "line", "pie", "scatter", "none"],
            "description": "Type of chart to visualize the data"
        },
        "confidence": {
            "type": "number",
            "minimum": 0,
            "maximum": 1,
            "description": "Confidence level in the answer (0.0 to 1.0)"
        },
        "metadata": {
            "type": "object",
            "description": "Optional metadata about the execution"
        }
    }
}
```

### 4. Error Handling

Two main exception types are provided:

- `SandboxExecutionError`: Raised when code execution fails
- `SchemaValidationError`: Raised when output doesn't conform to the schema

## Security Measures

The sandbox implements multiple layers of security:

1. **Process Isolation**: Code runs in a separate process that can be killed if it exceeds limits
2. **Memory Limitations**: Strict memory limits prevent resource exhaustion
3. **Timeouts**: Execution timeouts prevent infinite loops or long-running code
4. **Module Restrictions**: Only allowed modules can be imported
5. **System Call Blocking**: System calls and filesystem access are restricted

## Handling Retries

The system supports automatic retries with modified prompts when execution fails:

1. When an error occurs, the system captures the error details
2. A new prompt is created with the error information and instructions to fix the issue
3. The LLM is called again to generate corrected code
4. The corrected code is executed with the same validation requirements
5. If the retry fails, additional attempts may be made with increasingly specific instructions

## Usage Examples

### Basic Usage

```python
from src.utils.agent_sandbox import code_execution_in_sandbox

# Sample DataFrame
df = pd.DataFrame({
    'sales': [100, 150, 200],
    'cost': [80, 100, 120]
})

# Code to execute
code = """
df['profit'] = df['sales'] - df['cost']
total_profit = df['profit'].sum()

result = {
    "answer": f"Total profit is ${total_profit}",
    "data": {"total_profit": float(total_profit)},
    "chart_type": "none",
    "confidence": 0.9
}
"""

# Execute the code
result = code_execution_in_sandbox(code, df)
print(result['answer'])  # "Total profit is $150"
```

### With Error Handling and Retry

```python
try:
    result = code_execution_in_sandbox(
        code=code,
        df=df,
        llm_service_func=my_llm_service,
        original_prompt="Calculate the profit",
        enable_retry=True
    )
    print(result['answer'])
except SandboxExecutionError as e:
    print(f"Execution failed: {e}")
except SchemaValidationError as e:
    print(f"Invalid output format: {e}")
```

## Integration with LLM Systems

To integrate with an LLM code generation system:

1. Receive a user query
2. Call your LLM to generate Python code based on the query
3. Execute the generated code in the sandbox
4. If execution fails, retry with a modified prompt
5. Return the structured result to the user

Example integration flow:

```python
def process_query(query, dataframe):
    # Step 1: Generate code from the query
    code = llm_service.generate_code(query)
    
    # Step 2: Execute the code safely
    try:
        result = code_execution_in_sandbox(
            code=code,
            df=dataframe,
            llm_service_func=llm_service.generate_code,
            original_prompt=query,
            enable_retry=True
        )
        return result
    except Exception as e:
        # Handle any errors
        return {"error": str(e)}
```

## Best Practices

1. **Always validate inputs**: Ensure DataFrame inputs are clean and properly formatted
2. **Use custom schemas**: Define schemas specific to your application's needs
3. **Keep execution time limits reasonable**: 5-10 seconds is usually sufficient
4. **Customize retry prompts**: Include domain-specific guidance in retry prompts
5. **Log all executions**: Maintain audit logs of all code executions
6. **Set appropriate memory limits**: Adjust based on expected DataFrame sizes
7. **Test with malicious inputs**: Verify security measures with adversarial test cases

## Example Code and Tests

For working examples, see:
- `/examples/agent_sandbox_example.py`: Complete example of integrating with an LLM
- `/tests/unit/test_agent_sandbox.py`: Comprehensive test suite for the sandbox system

## Troubleshooting

Common issues and solutions:

1. **Execution timeouts**: Increase `execution_timeout_seconds` in SandboxConfig
2. **Memory errors**: Increase `memory_limit_mb` in SandboxConfig
3. **Missing modules**: Add required modules to `allowed_modules` in SandboxConfig
4. **Schema validation errors**: Check the output structure against the schema requirements
5. **Retry failures**: Ensure your LLM service function can generate corrected code

## Security Considerations

While the sandbox provides significant security benefits, be aware of these limitations:

1. It cannot prevent all forms of resource exhaustion
2. Complex mathematical operations might still cause high CPU usage
3. The sandbox focuses on preventing harmful side effects, not on ensuring code correctness
4. Regular security audits are recommended to identify potential vulnerabilities

## Future Enhancements

Planned improvements to the sandbox system:

1. Support for more granular resource controls
2. Integration with OpenTelemetry for observability
3. Enhanced visualization outputs (charts, tables, etc.)
4. Support for storing and reusing successful code patterns
5. Adaptive timeouts based on DataFrame size

---

For additional questions or support, contact the Watchdog AI development team.